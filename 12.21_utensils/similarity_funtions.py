import os
import torch
import torch.nn.functional as F
import numpy as np
from skimage.metrics import structural_similarity as ssim
from tqdm import tqdm
import multiprocessing
import psutil

from select_subj import load_subj_img_index

import datetime
import time
import gc

def get_current_time_info():
    now = datetime.datetime.now()
    standard_format = now.strftime("%m-%d_%H-%M-%S")
    return standard_format


# ================= é…ç½®åŒºåŸŸ =================
# æŒ‡å®šä½¿ç”¨é‚£å¼  48GB æ˜¾å­˜çš„æ˜¾å¡ (æ ¹æ®ä½ çš„ nvidia-smiï¼ŒGPU 4 æ˜¯ 48GB ç©ºé—²å¡)
os.environ["CUDA_VISIBLE_DEVICES"] = "2" 

# å…¨å±€å˜é‡ï¼šç”¨äºŽå¤šè¿›ç¨‹å…±äº«å†…å­˜ (Linux Fork æ¨¡å¼ä¸‹é›¶æ‹·è´)
global_things_img = None
global_nsd_img = None
co1 = np.array([10, 0.1, 0.1]) ## __adjustable__
co1 /= co1.sum()

# ================= å¤šè¿›ç¨‹ Worker å‡½æ•° =================
def calculate_fine_metrics_worker(args):
    """
    å•ä¸ªæŸ¥è¯¢çš„ç²¾ç»†è®¡ç®—ä»»åŠ¡
    args: (t_idx, candidate_indices, coarse_scores, top_r)
    """
    t_idx, candidate_indices, coarse_scores, top_r = args
    
    # ç›´æŽ¥è®¿é—®å…¨å±€å†…å­˜ï¼Œæ— éœ€æ‹·è´
    img_t = global_things_img[t_idx]       # (H, W, 3)
    imgs_n = global_nsd_img[candidate_indices] # (K, H, W, 3)
    
    fine_results = []
    
    # é¢„å¤„ç† Things å›¾ç‰‡ç”¨äºŽ PixCorr (å±•å¹³ & Center)
    flat_t = img_t.flatten().astype(np.float32)
    flat_t -= flat_t.mean()
    norm_t = np.linalg.norm(flat_t) + 1e-8
    
    for k_idx, img_n in enumerate(imgs_n):
        # --- 1. PixCorr è®¡ç®— ---
        flat_n = img_n.flatten().astype(np.float32)
        flat_n -= flat_n.mean()
        norm_n = np.linalg.norm(flat_n) + 1e-8
        
        # Pearson Correlation = Cosine of centered vectors
        pix_corr = np.dot(flat_t, flat_n) / (norm_t * norm_n)
        
        # --- 2. SSIM è®¡ç®— (æœ€è€—æ—¶) ---
        ssim_val = ssim(
            img_t, img_n, 
            channel_axis=2, 
            data_range=255,   # å‡è®¾è¾“å…¥æ˜¯ uint8 0-255
            win_size=11, 
            gaussian_weights=True, 
            sigma=1.5
        )
        
        # --- 3. æœ€ç»ˆåˆ†æ•°èžåˆ (Fusion) ---
        # è¿™é‡Œçš„æƒé‡ä½ å¯ä»¥æ ¹æ®åå¥½è°ƒæ•´
        # coarse_score æ˜¯å½’ä¸€åŒ–åŽçš„ Cosine Sim (é€šå¸¸ 0.x - 0.9)
        # ssim_val æ˜¯ 0 - 1
        # pix_corr æ˜¯ -1 - 1
        c_score = coarse_scores[k_idx]
        
        final_score = co1[0] * c_score + co1[1] * ssim_val + co1[2] * pix_corr
        
        fine_results.append((candidate_indices[k_idx], final_score, c_score, ssim_val, pix_corr))
    
    # æŒ‰æœ€ç»ˆåˆ†æ•°æŽ’åºï¼Œå– Top-R
    fine_results.sort(key=lambda x: x[1], reverse=True)
    top_r_res = fine_results[:top_r]
    
    # è¿”å›žç²¾ç®€ç»“æžœ: [things_idx, nsd_idx, final, coarse, ssim, pixcorr]
    return [[t_idx, int(r[0]), float(r[1]), float(r[2]), float(r[3]), float(r[4])] for r in top_r_res]

# ================= ä¸»ç³»ç»Ÿç±» =================
class RetrievalSystem:
    def __init__(self):
        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'
        print(f"ðŸš€ System initialized. Device: {self.device}")
        if torch.cuda.is_available():
            print(f"   GPU Name: {torch.cuda.get_device_name(0)}")
            
        # åŠ¨æ€è®¡ç®— CPU æ ¸æ•°
        self.num_workers = self.get_optimal_worker_count(reserve_cores=4)
        print(f"   CPU Workers: {self.num_workers} (Auto-configured)")

    def get_optimal_worker_count(self, reserve_cores=4):
        """æ ¹æ®ç³»ç»Ÿè´Ÿè½½åŠ¨æ€è®¡ç®—å¯ç”¨æ ¸æ•°"""
        try:
            total_cores = len(os.sched_getaffinity(0))
        except AttributeError:
            total_cores = os.cpu_count()
        try:
            load_1min, _, _ = os.getloadavg()
            current_load = int(load_1min)
        except OSError:
            current_load = 0
            
        return max(2, min(total_cores - current_load - reserve_cores, total_cores - 2))

    def load_images(self, things_path, nsd_path):
        global global_things_img, global_nsd_img
        print(f"ðŸ“¥ Loading raw images into RAM (Fast Mode)...")
        
        # å‡è®¾ .npy å­˜å‚¨çš„æ˜¯ uint8 (0-255)
        global_things_img = np.load(things_path)
        global_nsd_img = np.load(nsd_path)
        
        t_size = global_things_img.nbytes / (1024**3)
        n_size = global_nsd_img.nbytes / (1024**3)
        print(f"   Done. Things: {global_things_img.shape} ({t_size:.2f} GB)")
        print(f"         NSD:    {global_nsd_img.shape} ({n_size:.2f} GB)")
    def run_pipeline(self, feat_things, feat_nsd, weights, top_k=50, top_r=1, batch_size=4096):
        """
        feat_things/nsd: å­—å…¸ {'clip': np_array, 'alex2': np_array ...}
        weights: å­—å…¸ {'clip': 0.8, 'alex2': 0.2 ...}
        batch_size: 4096 (é’ˆå¯¹ 48GB æ˜¾å­˜ä¼˜åŒ–)
        """
        
        num_things = next(iter(feat_things.values())).shape[0]
        # éšä¾¿å–ä¸€ä¸ª nsd ç‰¹å¾çœ‹æ€»æ•°ï¼Œç”¨äºŽè®¡ç®— loop range
        num_nsd = next(iter(feat_nsd.values())).shape[0]
        
        final_results = []
        
        # NSD åˆ†å—å¤§å° (20000 * 140k * 4B â‰ˆ 10.4 GB)
        # åŠ ä¸Š Things çš„ 2GB å’Œç³»ç»Ÿå¼€é”€ï¼ŒæŽ§åˆ¶åœ¨ 15-20GB å·¦å³ï¼Œç»å¯¹å®‰å…¨
        NSD_CHUNK_SIZE = 20000
        
        print(f"ðŸ”¥ Starting Retrieval Loop (A-Batch={batch_size}, NSD-Chunk={NSD_CHUNK_SIZE}, Top-K={top_k})...")
        
        # --- Phase 2: Coarse Search (Matrix Multiplication) ---
        for i in tqdm(range(0, num_things, batch_size), desc="Processing Batches"):
            start = i
            end = min(i + batch_size, num_things)
            curr_bs = end - start
            
            # åˆå§‹åŒ–å½“å‰ Batch çš„æ€»åˆ†çŸ©é˜µ (Batch_Size, Num_NSD)
            # æ³¨æ„ï¼šè¿™ä¸ªçŸ©é˜µåªæœ‰ 4096 x 73000 x 4B â‰ˆ 1.1GBï¼Œå¸¸é©»æ˜¾å­˜å®Œå…¨æ²¡é—®é¢˜
            total_sim = torch.zeros((curr_bs, num_nsd), device=self.device)
            
            # ç´¯åŠ å„ç‰¹å¾åˆ†æ•°
            for key, w in weights.items():
                if w == 0: continue
                
                # 1. å‡†å¤‡ Things Batch (A) - å¸¸é©»æ˜¾å­˜
                # å³ä½¿æ˜¯ Alex2ï¼Œ4096 å¼ ä¹Ÿæ‰ 2GBï¼Œå¯ä»¥æŽ¥å—
                ft_things = torch.from_numpy(feat_things[key][start:end]).float()
                ft_things = ft_things.contiguous().to(self.device)
                ft_things = F.normalize(ft_things, p=2, dim=1)
                
                # 2. å‡†å¤‡ NSD æ•°æ®æº (B) - ä»åœ¨ CPU å†…å­˜ä¸­
                nsd_source_cpu = feat_nsd[key]
                
                # 3. åˆ†å—è®¡ç®— NSD ç›¸ä¼¼åº¦
                sim_chunks = [] # ç”¨äºŽæš‚å­˜å„å—çš„è®¡ç®—ç»“æžœ
                
                for n_start in range(0, num_nsd, NSD_CHUNK_SIZE):
                    n_end = min(n_start + NSD_CHUNK_SIZE, num_nsd)
                    
                    # [æ¬è¿] åˆ‡ç‰‡ -> æ¬å…¥ GPU (æœ€å¤šå ç”¨ 10GB)
                    ft_nsd_chunk = torch.from_numpy(nsd_source_cpu[n_start:n_end]).float()
                    ft_nsd_chunk = ft_nsd_chunk.contiguous().to(self.device)
                    ft_nsd_chunk = F.normalize(ft_nsd_chunk, p=2, dim=1)
                    
                    # [è®¡ç®—] (Batch_A, Dim) @ (Dim, Chunk_B) -> (Batch_A, Chunk_B)
                    # ç»“æžœçŸ©é˜µå¾ˆå°ï¼Œä¸ç”¨æ‹…å¿ƒ
                    chunk_sim = torch.matmul(ft_things, ft_nsd_chunk.T)
                    
                    # [æš‚å­˜]
                    sim_chunks.append(chunk_sim)
                    
                    # [é‡Šæ”¾] æ˜¾å¼åˆ é™¤å¼•ç”¨ï¼Œç¡®ä¿æ˜¾å­˜å›žæ”¶ç»™ä¸‹ä¸€å—
                    del ft_nsd_chunk
                
                # 4. æ‹¼åˆ & åŠ æƒ
                # æŠŠæ‰€æœ‰å°å—æ‹¼æˆå®Œæ•´çš„ (Batch_A, Num_NSD)
                full_sim_matrix = torch.cat(sim_chunks, dim=1)
                total_sim += w * full_sim_matrix
                
                # é‡Šæ”¾ Things ç‰¹å¾
                del ft_things
                # é‡Šæ”¾ä¸­é—´ç»“æžœ
                del full_sim_matrix, sim_chunks

            # --- è‡³æ­¤ï¼Œå½“å‰ Batch A çš„æ‰€æœ‰ç‰¹å¾åŠ æƒæ€»åˆ†å·²ç®—å‡º ---
            
            # GPU ä¸Šç›´æŽ¥å– Top-K
            top_k_scores, top_k_indices = torch.topk(total_sim, k=top_k, dim=1)
            
            # è½¬ CPU å‡†å¤‡ç²¾æŽ’
            indices_np = top_k_indices.cpu().numpy()
            scores_np = top_k_scores.cpu().numpy()
            
            # é‡Šæ”¾ total_simï¼Œè…¾å‡ºç©ºé—´ç»™ä¸‹ä¸€ä¸ª Batch
            del total_sim
            
            # --- Phase 3: Fine Re-ranking (Multiprocessing CPU) ---
            # å‡†å¤‡ä»»åŠ¡åˆ—è¡¨
            tasks = []
            for b in range(curr_bs):
                t_idx = start + b
                # å°†è¯¥ query çš„ä»»åŠ¡æ‰“åŒ…
                tasks.append((t_idx, indices_np[b], scores_np[b], top_r))
            
            # å¯åŠ¨è¿›ç¨‹æ± 
            current_workers = self.get_optimal_worker_count(reserve_cores=4)
            
            with multiprocessing.Pool(processes=current_workers) as pool:
                # å¹¶è¡Œè®¡ç®— SSIM & PixCorr
                batch_fine_results = pool.map(calculate_fine_metrics_worker, tasks)
            
            # æ”¶é›†ç»“æžœ
            for res in batch_fine_results:
                final_results.extend(res)

        return final_results
import sys
import json
if __name__ == "__main__":
    # 1. å®žä¾‹åŒ–ç³»ç»Ÿ
    system = RetrievalSystem()
    # 2. è·¯å¾„é…ç½® (è¯·ä¿®æ”¹ä¸ºä½ çš„å®žé™…è·¯å¾„)
    base_path = "/home/ysunem/12.21/THINGS-NSD_code/" # ä½ çš„æ•°æ®ç›®å½•
    things_npy = os.path.join(base_path, "things_img.npy") # å¿…é¡»æ˜¯ uint8
    nsd_npy = os.path.join(base_path, "nsd_img.npy")       # å¿…é¡»æ˜¯ uint8
    
    # å¦‚æžœæ–‡ä»¶ä¸å­˜åœ¨ï¼Œè¯·å…ˆç¡®ä¿è·¯å¾„æ­£ç¡®
    if os.path.exists(things_npy):
        system.load_images(things_npy, nsd_npy)
    else:
        print("âš ï¸ Warning: Image files not found. Skipping image loading (Demo mode).")
    if global_things_img.dtype != np.uint8 or global_nsd_img.dtype != np.uint8:
        print("âŒ Error: Image .npy files must be of dtype uint8 (0-255).")
        sys.exit(1)


    # 4. æ¨¡æ‹Ÿ/åŠ è½½ Embedding æ•°æ®
    # åŠ è½½åˆšæ‰ç”Ÿæˆçš„ç‰¹å¾
    print("ðŸ“¥ Loading Precomputed Embeddings...")

    # ä½ çš„ CLIP ç‰¹å¾ (å‡è®¾ä½ æœ¬æ¥å°±æœ‰)
    things_clip = np.load(base_path + 'things_fea.npy').astype(np.float32)
    nsd_clip = np.load(base_path + 'nsd_fea.npy').astype(np.float32)
    
    feat_things_dict = np.load(base_path + 'features/feat_things.npy', allow_pickle=True).item()
    feat_nsd_dict = np.load(base_path + 'features/feat_nsd.npy', allow_pickle=True).item()

    # åˆå¹¶å­—å…¸
    feat_things = feat_things_dict
    feat_things['clip'] = things_clip 

    feat_nsd = feat_nsd_dict
    feat_nsd['clip'] = nsd_clip 
    

    # 5. å®šä¹‰ç²—ç­›æƒé‡ (Coarse Weights)
    co = np.array([7, 3, 3, 0.1, 3, 0.1]) ## __adjustable__
    co /= np.sum(co)
    weights = {
        'clip': co[0], 
        'alex2': co[1],  
        'alex5': co[2],  
        'incep': co[3],
        'eff': co[4],    
        'swav': co[5]
    }

    keep_index = load_subj_img_index(subj_id1=1) ## $$$
    for k, v in feat_nsd.items():
        feat_nsd[k] = v[keep_index]
    global_nsd_img = global_nsd_img[keep_index] 
    
    gc.collect()
    # 6. è¿è¡Œ Pipeline
    results = system.run_pipeline(feat_things, feat_nsd, weights, top_k=50, top_r=5, batch_size=4096)


    # --- é…ç½®é˜ˆå€¼ (æ ¹æ®ä½ çš„å®žé™…åˆ†æ•°åˆ†å¸ƒè°ƒæ•´) ---
    # æç¤ºï¼šFinal Score æ˜¯å½’ä¸€åŒ–çš„ï¼Œç†è®ºæœ€å¤§å€¼æ˜¯ 1.0
    THRESHOLD_1 = None  # é«˜ç½®ä¿¡åº¦ (High Confidence)
    THRESHOLD_2 = None  # ä¸­ç½®ä¿¡åº¦ (Medium Confidence)


    # ================= 6.5. ç»˜åˆ¶åˆ†æ•°åˆ†å¸ƒç›´æ–¹å›¾ =================
    import matplotlib.pyplot as plt
    
    print("ðŸ“ˆ Plotting score distributions...")
    
    # 1. æå–æ•°æ® (è½¬ä¸º Numpy æ–¹ä¾¿åˆ‡ç‰‡)
    # resultsç»“æž„: [t_idx, n_idx, final, coarse, ssim, pixcorr]
    data_arr = np.array(results) 
    
    scores_final = data_arr[:, 2]
    scores_coarse = data_arr[:, 3]
    scores_ssim = data_arr[:, 4]
    scores_pixcorr = data_arr[:, 5]
    THRESHOLD_1, THRESHOLD_2 = np.percentile(scores_final, [90, 70]) ## __adjustable__
    
    try :
        scores_struct = (scores_ssim * co1[1] + scores_pixcorr * co1[2]) / (co1[1] + co1[2])
    except:
        scores_struct = (scores_ssim * 0.3 + scores_pixcorr * 0.1) / 0.4
    # 3. åˆ›å»ºç”»å¸ƒ
    fig, axes = plt.subplots(1, 3, figsize=(18, 5))
    
    # --- Plot 1: Coarse Score (è¯­ä¹‰ç›¸ä¼¼åº¦) ---
    axes[0].hist(scores_coarse, bins=100, color='skyblue', edgecolor='black', alpha=0.7)
    axes[0].set_title('Coarse Score (Deep Features)', fontsize=14)
    axes[0].set_xlabel('Score')
    axes[0].set_ylabel('Count')
    axes[0].grid(axis='y', alpha=0.5)

    # --- Plot 2: Structural Score (SSIM & PixCorr) ---
    axes[1].hist(scores_struct, bins=100, color='lightgreen', edgecolor='black', alpha=0.7)
    axes[1].set_title('Structural Score\n(0.3*SSIM + 0.1*Pix) / 0.4', fontsize=14)
    axes[1].set_xlabel('Score')
    axes[1].grid(axis='y', alpha=0.5)

    # --- Plot 3: Final Score (æœ€ç»ˆèžåˆ) ---
    axes[2].hist(scores_final, bins=100, color='salmon', edgecolor='black', alpha=0.7)
    axes[2].set_title('Final Weighted Score', fontsize=14)
    axes[2].set_xlabel('Score')
    axes[2].grid(axis='y', alpha=0.5)

    # (å¯é€‰) åœ¨ Final Score å›¾ä¸Šç”»å‡ºä½ é¢„æƒ³çš„ Threshold è¾…åŠ©çº¿
    # å¸®ä½ åˆ¤æ–­ T1 å’Œ T2 åˆ‡åœ¨å“ªé‡Œåˆé€‚
    axes[2].axvline(THRESHOLD_1, color='red', linestyle='dashed', linewidth=2, label='T1 (High)')
    axes[2].axvline(THRESHOLD_2, color='blue', linestyle='dashed', linewidth=2, label='T2 (Med)')
    axes[2].legend()

    plt.tight_layout()
    
    # ä¿å­˜å›¾ç‰‡
    plt.savefig("score_distribution.png", dpi=300)
    print("âœ… Histogram saved to score_distribution.png. Check this file to set thresholds!")
    
    # 7. ç»“æžœè¿‡æ»¤ä¸Žä¿å­˜ (JSON æ ¼å¼)

    print(f"ðŸ“Š Filtering results with T1={THRESHOLD_1}, T2={THRESHOLD_2}...")

    # åˆå§‹åŒ– JSON ç»“æž„
    json_output = {
        "metadata": {
            "total_queries": len(results),
            "threshold_high": THRESHOLD_1,
            "threshold_medium": THRESHOLD_2,
            "weights": weights
        },
        "high_confidence_group": [],   # > T1
        "medium_confidence_group": [],  # T2 <= score < T1
        "discarded": []  # score < T2
    }

    # è®¡æ•°å™¨
    count_high = 0
    count_med = 0
    count_discard = 0

    for row in results:
        # row æ ¼å¼: [t_idx, n_idx, final, coarse, ssim, pixcorr]
        # æ³¨æ„ï¼šè¿™é‡Œå¿…é¡»è½¬ä¸º Python åŽŸç”Ÿç±»åž‹ (float/int)ï¼Œå¦åˆ™ json.dump ä¼šæŠ¥é”™
        item = {
            "things_id": int(row[0]),
            "nsd_id": int(row[1]),
            "score_final": round(float(row[2]), 4),
            "score_coarse": round(float(row[3]), 4),
            "score_ssim": round(float(row[4]), 4),
            "score_pixcorr": round(float(row[5]), 4)
        }

        score = item["score_final"]

        if score >= THRESHOLD_1:
            json_output["high_confidence_group"].append(item)
            count_high += 1
        elif score >= THRESHOLD_2:
            json_output["medium_confidence_group"].append(item)
            count_med += 1
        else:
            json_output["discarded"].append(item)
            count_discard += 1

    # ä¿å­˜æ–‡ä»¶
    output_json_path = f"retrieval_filtered{get_current_time_info()}.json"
    with open(output_json_path, 'w') as f:
        json.dump(json_output, f, indent=4) 

    print(f"âœ… Filtered JSON saved to {output_json_path}")
    print(f"   - High Confidence (> {THRESHOLD_1}): {count_high} pairs")
    print(f"   - Medium Confidence ([{THRESHOLD_2}, {THRESHOLD_1})): {count_med} pairs")
    print(f"   - Discarded (< {THRESHOLD_2}): {count_discard} pairs")

    import pandas as pd
    pd.DataFrame(results).to_csv(f"retrieval_raw_backup{get_current_time_info()}.csv", index=False)